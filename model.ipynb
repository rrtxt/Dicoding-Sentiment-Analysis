{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd36af86",
   "metadata": {},
   "source": [
    "## Proyek Analisis Sentimen Dicoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b91404a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db4ad349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b5691293-970e-422e-a708-208e4cf5744e</td>\n",
       "      <td>Baguss banget Aplikasinya âœ¨ï¸ğŸ”¥ sangat membantu ...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-04-23 14:23:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>558f02c7-4fa5-4086-b70d-4513b23bc989</td>\n",
       "      <td>Duolingo memang baguss polll, aku suka banget....</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>2025-04-05 12:42:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29e68c3d-cd29-404d-b63b-21cbb0bf06f8</td>\n",
       "      <td>Aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>2025-04-22 05:46:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9803c43e-346d-4ecd-bd50-3d794cb3fc4c</td>\n",
       "      <td>Untuk Developer, Tolong dong saat kita melakuk...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-22 00:24:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a0caa63f-f6e5-49e7-a843-e996e22c4b25</td>\n",
       "      <td>Suka bgtt sma Duolingo ini, belajar jdi lebih ...</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2025-04-22 19:08:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId  \\\n",
       "0  b5691293-970e-422e-a708-208e4cf5744e   \n",
       "1  558f02c7-4fa5-4086-b70d-4513b23bc989   \n",
       "2  29e68c3d-cd29-404d-b63b-21cbb0bf06f8   \n",
       "3  9803c43e-346d-4ecd-bd50-3d794cb3fc4c   \n",
       "4  a0caa63f-f6e5-49e7-a843-e996e22c4b25   \n",
       "\n",
       "                                              review  score  thumbsUpCount  \\\n",
       "0  Baguss banget Aplikasinya âœ¨ï¸ğŸ”¥ sangat membantu ...      5              6   \n",
       "1  Duolingo memang baguss polll, aku suka banget....      3             36   \n",
       "2  Aplikasi keren saya banyak belajar lewat aplik...      5             43   \n",
       "3  Untuk Developer, Tolong dong saat kita melakuk...      4              5   \n",
       "4  Suka bgtt sma Duolingo ini, belajar jdi lebih ...      5             18   \n",
       "\n",
       "                    at  \n",
       "0  2025-04-23 14:23:11  \n",
       "1  2025-04-05 12:42:35  \n",
       "2  2025-04-22 05:46:12  \n",
       "3  2025-04-22 00:24:08  \n",
       "4  2025-04-22 19:08:05  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/duolingo_review.csv\")\n",
    "df.drop(columns=[\"userName\", \"userImage\", \"replyContent\", \"repliedAt\", \"appVersion\", \"reviewCreatedVersion\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239ffab",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6a73a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   reviewId       18000 non-null  object\n",
      " 1   review         18000 non-null  object\n",
      " 2   score          18000 non-null  int64 \n",
      " 3   thumbsUpCount  18000 non-null  int64 \n",
      " 4   at             18000 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 703.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Clean Dataset\n",
    "clean_df = df.dropna()\n",
    "\n",
    "# Drop Duplicate\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e71e0",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93b98e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad51d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)  # menggantikan huruf yang berulang menjadi satu huruf\n",
    "    \n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    " \n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    " \n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    " \n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    " \n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    " \n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    " \n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    " \n",
    "    return stemmed_text\n",
    " \n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence\n",
    "\n",
    "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\"}\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "729ab40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "clean_df['text_clean'] = clean_df['review'].apply(cleaningText)\n",
    " \n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)\n",
    " \n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)\n",
    " \n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)\n",
    " \n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)\n",
    " \n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c459822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>at</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b5691293-970e-422e-a708-208e4cf5744e</td>\n",
       "      <td>Baguss banget Aplikasinya âœ¨ï¸ğŸ”¥ sangat membantu ...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-04-23 14:23:11</td>\n",
       "      <td>Bagus banget Aplikasinya sangat membantu dalam...</td>\n",
       "      <td>bagus banget aplikasinya sangat membantu dalam...</td>\n",
       "      <td>bagus banget aplikasinya sangat membantu dalam...</td>\n",
       "      <td>[bagus, banget, aplikasinya, sangat, membantu,...</td>\n",
       "      <td>[bagus, banget, aplikasinya, membantu, mengasa...</td>\n",
       "      <td>bagus banget aplikasinya membantu mengasah ski...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>558f02c7-4fa5-4086-b70d-4513b23bc989</td>\n",
       "      <td>Duolingo memang baguss polll, aku suka banget....</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>2025-04-05 12:42:35</td>\n",
       "      <td>Duolingo memang bagus pol aku suka banget yang...</td>\n",
       "      <td>duolingo memang bagus pol aku suka banget yang...</td>\n",
       "      <td>duolingo memang bagus pol aku suka banget yang...</td>\n",
       "      <td>[duolingo, memang, bagus, pol, aku, suka, bang...</td>\n",
       "      <td>[duolingo, bagus, pol, suka, banget, akhirÂ², i...</td>\n",
       "      <td>duolingo bagus pol suka banget akhirÂ² iklanya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29e68c3d-cd29-404d-b63b-21cbb0bf06f8</td>\n",
       "      <td>Aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>2025-04-22 05:46:12</td>\n",
       "      <td>Aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>[aplikasi, keren, saya, banyak, belajar, lewat...</td>\n",
       "      <td>[aplikasi, keren, belajar, aplikasi, kekuranga...</td>\n",
       "      <td>aplikasi keren belajar aplikasi kekuranganya s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9803c43e-346d-4ecd-bd50-3d794cb3fc4c</td>\n",
       "      <td>Untuk Developer, Tolong dong saat kita melakuk...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-22 00:24:08</td>\n",
       "      <td>Untuk Developer Tolong dong sat kita melakukan...</td>\n",
       "      <td>untuk developer tolong dong sat kita melakukan...</td>\n",
       "      <td>untuk developer tolong dong sat kita melakukan...</td>\n",
       "      <td>[untuk, developer, tolong, dong, sat, kita, me...</td>\n",
       "      <td>[developer, tolong, sat, kesalahan, kekurangan...</td>\n",
       "      <td>developer tolong sat kesalahan kekurangan huru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a0caa63f-f6e5-49e7-a843-e996e22c4b25</td>\n",
       "      <td>Suka bgtt sma Duolingo ini, belajar jdi lebih ...</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2025-04-22 19:08:05</td>\n",
       "      <td>Suka bgt sma Duolingo ini belajar jdi lebih se...</td>\n",
       "      <td>suka bgt sma duolingo ini belajar jdi lebih se...</td>\n",
       "      <td>suka banget sma duolingo ini belajar jdi lebih...</td>\n",
       "      <td>[suka, banget, sma, duolingo, ini, belajar, jd...</td>\n",
       "      <td>[suka, banget, sma, duolingo, belajar, jdi, se...</td>\n",
       "      <td>suka banget sma duolingo belajar jdi seru muda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId  \\\n",
       "0  b5691293-970e-422e-a708-208e4cf5744e   \n",
       "1  558f02c7-4fa5-4086-b70d-4513b23bc989   \n",
       "2  29e68c3d-cd29-404d-b63b-21cbb0bf06f8   \n",
       "3  9803c43e-346d-4ecd-bd50-3d794cb3fc4c   \n",
       "4  a0caa63f-f6e5-49e7-a843-e996e22c4b25   \n",
       "\n",
       "                                              review  score  thumbsUpCount  \\\n",
       "0  Baguss banget Aplikasinya âœ¨ï¸ğŸ”¥ sangat membantu ...      5              6   \n",
       "1  Duolingo memang baguss polll, aku suka banget....      3             36   \n",
       "2  Aplikasi keren saya banyak belajar lewat aplik...      5             43   \n",
       "3  Untuk Developer, Tolong dong saat kita melakuk...      4              5   \n",
       "4  Suka bgtt sma Duolingo ini, belajar jdi lebih ...      5             18   \n",
       "\n",
       "                    at                                         text_clean  \\\n",
       "0  2025-04-23 14:23:11  Bagus banget Aplikasinya sangat membantu dalam...   \n",
       "1  2025-04-05 12:42:35  Duolingo memang bagus pol aku suka banget yang...   \n",
       "2  2025-04-22 05:46:12  Aplikasi keren saya banyak belajar lewat aplik...   \n",
       "3  2025-04-22 00:24:08  Untuk Developer Tolong dong sat kita melakukan...   \n",
       "4  2025-04-22 19:08:05  Suka bgt sma Duolingo ini belajar jdi lebih se...   \n",
       "\n",
       "                                text_casefoldingText  \\\n",
       "0  bagus banget aplikasinya sangat membantu dalam...   \n",
       "1  duolingo memang bagus pol aku suka banget yang...   \n",
       "2  aplikasi keren saya banyak belajar lewat aplik...   \n",
       "3  untuk developer tolong dong sat kita melakukan...   \n",
       "4  suka bgt sma duolingo ini belajar jdi lebih se...   \n",
       "\n",
       "                                     text_slangwords  \\\n",
       "0  bagus banget aplikasinya sangat membantu dalam...   \n",
       "1  duolingo memang bagus pol aku suka banget yang...   \n",
       "2  aplikasi keren saya banyak belajar lewat aplik...   \n",
       "3  untuk developer tolong dong sat kita melakukan...   \n",
       "4  suka banget sma duolingo ini belajar jdi lebih...   \n",
       "\n",
       "                                 text_tokenizingText  \\\n",
       "0  [bagus, banget, aplikasinya, sangat, membantu,...   \n",
       "1  [duolingo, memang, bagus, pol, aku, suka, bang...   \n",
       "2  [aplikasi, keren, saya, banyak, belajar, lewat...   \n",
       "3  [untuk, developer, tolong, dong, sat, kita, me...   \n",
       "4  [suka, banget, sma, duolingo, ini, belajar, jd...   \n",
       "\n",
       "                                       text_stopword  \\\n",
       "0  [bagus, banget, aplikasinya, membantu, mengasa...   \n",
       "1  [duolingo, bagus, pol, suka, banget, akhirÂ², i...   \n",
       "2  [aplikasi, keren, belajar, aplikasi, kekuranga...   \n",
       "3  [developer, tolong, sat, kesalahan, kekurangan...   \n",
       "4  [suka, banget, sma, duolingo, belajar, jdi, se...   \n",
       "\n",
       "                                          text_akhir  \n",
       "0  bagus banget aplikasinya membantu mengasah ski...  \n",
       "1  duolingo bagus pol suka banget akhirÂ² iklanya ...  \n",
       "2  aplikasi keren belajar aplikasi kekuranganya s...  \n",
       "3  developer tolong sat kesalahan kekurangan huru...  \n",
       "4  suka banget sma duolingo belajar jdi seru muda...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c76f6c",
   "metadata": {},
   "source": [
    "### Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7fcec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c5c64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive\n",
    "else:\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    " \n",
    "# Membaca data kamus kata-kata negatif dari GitHub\n",
    "lexicon_negative = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative\n",
    "else:\n",
    "    print(\"Failed to fetch negative lexicon data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60b76bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rizfi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd9a9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    " \n",
    "    score = 0\n",
    "    # Inisialisasi skor sentimen ke 0\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks\n",
    " \n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks (sekali lagi)\n",
    " \n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen\n",
    " \n",
    "    polarity=''\n",
    "    # Inisialisasi variabel polaritas\n",
    " \n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    " \n",
    "    return score, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27316807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analysis_lexicon_vader(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Extract individual scores\n",
    "    neg_score = scores[\"neg\"]\n",
    "    neu_score = scores[\"neu\"]\n",
    "    pos_score = scores[\"pos\"]\n",
    "    compound = scores[\"compound\"]\n",
    "    \n",
    "    # Determine polarity based on the highest score\n",
    "    if neu_score > neg_score and neu_score > pos_score:\n",
    "        polarity = \"neutral\"\n",
    "    elif pos_score > neg_score:\n",
    "        polarity = \"positive\"\n",
    "    else:\n",
    "        polarity = \"negative\"\n",
    "    \n",
    "    return compound, polarity, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c39e0b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity\n",
       "negative    10057\n",
       "positive     6670\n",
       "neutral      1273\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "# results = clean_df[\"text_akhir\"].apply(sentiment_analysis_lexicon_vader)\n",
    "results = list(zip(*results))\n",
    "clean_df['polarity_score'] = results[0]\n",
    "clean_df['polarity'] = results[1]\n",
    "clean_df[\"polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4843f46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>at</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b5691293-970e-422e-a708-208e4cf5744e</td>\n",
       "      <td>Baguss banget Aplikasinya âœ¨ï¸ğŸ”¥ sangat membantu ...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-04-23 14:23:11</td>\n",
       "      <td>Bagus banget Aplikasinya sangat membantu dalam...</td>\n",
       "      <td>bagus banget aplikasinya sangat membantu dalam...</td>\n",
       "      <td>bagus banget aplikasinya sangat membantu dalam...</td>\n",
       "      <td>[bagus, banget, aplikasinya, sangat, membantu,...</td>\n",
       "      <td>[bagus, banget, aplikasinya, membantu, mengasa...</td>\n",
       "      <td>bagus banget aplikasinya membantu mengasah ski...</td>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>558f02c7-4fa5-4086-b70d-4513b23bc989</td>\n",
       "      <td>Duolingo memang baguss polll, aku suka banget....</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>2025-04-05 12:42:35</td>\n",
       "      <td>Duolingo memang bagus pol aku suka banget yang...</td>\n",
       "      <td>duolingo memang bagus pol aku suka banget yang...</td>\n",
       "      <td>duolingo memang bagus pol aku suka banget yang...</td>\n",
       "      <td>[duolingo, memang, bagus, pol, aku, suka, bang...</td>\n",
       "      <td>[duolingo, bagus, pol, suka, banget, akhirÂ², i...</td>\n",
       "      <td>duolingo bagus pol suka banget akhirÂ² iklanya ...</td>\n",
       "      <td>-2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29e68c3d-cd29-404d-b63b-21cbb0bf06f8</td>\n",
       "      <td>Aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>2025-04-22 05:46:12</td>\n",
       "      <td>Aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>aplikasi keren saya banyak belajar lewat aplik...</td>\n",
       "      <td>[aplikasi, keren, saya, banyak, belajar, lewat...</td>\n",
       "      <td>[aplikasi, keren, belajar, aplikasi, kekuranga...</td>\n",
       "      <td>aplikasi keren belajar aplikasi kekuranganya s...</td>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9803c43e-346d-4ecd-bd50-3d794cb3fc4c</td>\n",
       "      <td>Untuk Developer, Tolong dong saat kita melakuk...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-22 00:24:08</td>\n",
       "      <td>Untuk Developer Tolong dong sat kita melakukan...</td>\n",
       "      <td>untuk developer tolong dong sat kita melakukan...</td>\n",
       "      <td>untuk developer tolong dong sat kita melakukan...</td>\n",
       "      <td>[untuk, developer, tolong, dong, sat, kita, me...</td>\n",
       "      <td>[developer, tolong, sat, kesalahan, kekurangan...</td>\n",
       "      <td>developer tolong sat kesalahan kekurangan huru...</td>\n",
       "      <td>-26</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a0caa63f-f6e5-49e7-a843-e996e22c4b25</td>\n",
       "      <td>Suka bgtt sma Duolingo ini, belajar jdi lebih ...</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2025-04-22 19:08:05</td>\n",
       "      <td>Suka bgt sma Duolingo ini belajar jdi lebih se...</td>\n",
       "      <td>suka bgt sma duolingo ini belajar jdi lebih se...</td>\n",
       "      <td>suka banget sma duolingo ini belajar jdi lebih...</td>\n",
       "      <td>[suka, banget, sma, duolingo, ini, belajar, jd...</td>\n",
       "      <td>[suka, banget, sma, duolingo, belajar, jdi, se...</td>\n",
       "      <td>suka banget sma duolingo belajar jdi seru muda...</td>\n",
       "      <td>12</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId  \\\n",
       "0  b5691293-970e-422e-a708-208e4cf5744e   \n",
       "1  558f02c7-4fa5-4086-b70d-4513b23bc989   \n",
       "2  29e68c3d-cd29-404d-b63b-21cbb0bf06f8   \n",
       "3  9803c43e-346d-4ecd-bd50-3d794cb3fc4c   \n",
       "4  a0caa63f-f6e5-49e7-a843-e996e22c4b25   \n",
       "\n",
       "                                              review  score  thumbsUpCount  \\\n",
       "0  Baguss banget Aplikasinya âœ¨ï¸ğŸ”¥ sangat membantu ...      5              6   \n",
       "1  Duolingo memang baguss polll, aku suka banget....      3             36   \n",
       "2  Aplikasi keren saya banyak belajar lewat aplik...      5             43   \n",
       "3  Untuk Developer, Tolong dong saat kita melakuk...      4              5   \n",
       "4  Suka bgtt sma Duolingo ini, belajar jdi lebih ...      5             18   \n",
       "\n",
       "                    at                                         text_clean  \\\n",
       "0  2025-04-23 14:23:11  Bagus banget Aplikasinya sangat membantu dalam...   \n",
       "1  2025-04-05 12:42:35  Duolingo memang bagus pol aku suka banget yang...   \n",
       "2  2025-04-22 05:46:12  Aplikasi keren saya banyak belajar lewat aplik...   \n",
       "3  2025-04-22 00:24:08  Untuk Developer Tolong dong sat kita melakukan...   \n",
       "4  2025-04-22 19:08:05  Suka bgt sma Duolingo ini belajar jdi lebih se...   \n",
       "\n",
       "                                text_casefoldingText  \\\n",
       "0  bagus banget aplikasinya sangat membantu dalam...   \n",
       "1  duolingo memang bagus pol aku suka banget yang...   \n",
       "2  aplikasi keren saya banyak belajar lewat aplik...   \n",
       "3  untuk developer tolong dong sat kita melakukan...   \n",
       "4  suka bgt sma duolingo ini belajar jdi lebih se...   \n",
       "\n",
       "                                     text_slangwords  \\\n",
       "0  bagus banget aplikasinya sangat membantu dalam...   \n",
       "1  duolingo memang bagus pol aku suka banget yang...   \n",
       "2  aplikasi keren saya banyak belajar lewat aplik...   \n",
       "3  untuk developer tolong dong sat kita melakukan...   \n",
       "4  suka banget sma duolingo ini belajar jdi lebih...   \n",
       "\n",
       "                                 text_tokenizingText  \\\n",
       "0  [bagus, banget, aplikasinya, sangat, membantu,...   \n",
       "1  [duolingo, memang, bagus, pol, aku, suka, bang...   \n",
       "2  [aplikasi, keren, saya, banyak, belajar, lewat...   \n",
       "3  [untuk, developer, tolong, dong, sat, kita, me...   \n",
       "4  [suka, banget, sma, duolingo, ini, belajar, jd...   \n",
       "\n",
       "                                       text_stopword  \\\n",
       "0  [bagus, banget, aplikasinya, membantu, mengasa...   \n",
       "1  [duolingo, bagus, pol, suka, banget, akhirÂ², i...   \n",
       "2  [aplikasi, keren, belajar, aplikasi, kekuranga...   \n",
       "3  [developer, tolong, sat, kesalahan, kekurangan...   \n",
       "4  [suka, banget, sma, duolingo, belajar, jdi, se...   \n",
       "\n",
       "                                          text_akhir  polarity_score  polarity  \n",
       "0  bagus banget aplikasinya membantu mengasah ski...               6  positive  \n",
       "1  duolingo bagus pol suka banget akhirÂ² iklanya ...              -2  negative  \n",
       "2  aplikasi keren belajar aplikasi kekuranganya s...               7  positive  \n",
       "3  developer tolong sat kesalahan kekurangan huru...             -26  negative  \n",
       "4  suka banget sma duolingo belajar jdi seru muda...              12  positive  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b71ac9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "x = clean_df[\"text_akhir\"]\n",
    "y = clean_df[\"polarity\"]\n",
    "encoded_label = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, encoded_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c63a7",
   "metadata": {},
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68ae949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_x_train = tfidf.fit_transform(x_train)\n",
    "tfidf_x_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed5018",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5527c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rizfi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "X_train_processed = [simple_preprocess(text) for text in x_train]\n",
    "X_test_processed = [simple_preprocess(text) for text in x_test]\n",
    "\n",
    "w2v_model = Word2Vec(sentences=X_train_processed, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_document_vector(doc, model):\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "x_train_vec = np.array([get_document_vector(doc, w2v_model) for doc in X_train_processed])\n",
    "x_test_vec = np.array([get_document_vector(doc, w2v_model) for doc in X_test_processed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab236c23",
   "metadata": {},
   "source": [
    "#### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73b91fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "bow_x_train = vectorizer.fit_transform(x_train)\n",
    "bow_x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73f79a",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ba7b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35ebcb4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mhdd\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'hdd' is not defined"
     ]
    }
   ],
   "source": [
    "hdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad412ce",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes (TFIDF) - accuracy_train: 0.8264583333333333\n",
      "Naive Bayes (TFIDF) - accuracy_test: 0.735\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Using TFIDF\n",
    "naive_bayes_tfidf = BernoulliNB()\n",
    "\n",
    "naive_bayes_tfidf.fit(tfidf_x_train, y_train)\n",
    "\n",
    "y_pred_train_nb = naive_bayes_tfidf.predict(tfidf_x_train)\n",
    "y_pred_test_nb = naive_bayes_tfidf.predict(tfidf_x_test)\n",
    "\n",
    "accuracy_train_nb = accuracy_score(y_pred_train_nb, y_train)\n",
    "accuracy_test_nb = accuracy_score(y_pred_test_nb, y_test)\n",
    " \n",
    "print('Naive Bayes (TFIDF) - accuracy_train:', accuracy_train_nb)\n",
    "print('Naive Bayes (TFIDF) - accuracy_test:', accuracy_test_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfac81",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9c22a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy_train: 0.7273611111111111\n",
      "Logistic Regression - accuracy_test: 0.7266666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "logistic_regression.fit(x_train_vec, y_train)\n",
    "\n",
    "y_pred_train_lr = logistic_regression.predict(x_train_vec)\n",
    "y_pred_test_lr = logistic_regression.predict(x_test_vec)\n",
    "\n",
    "accuracy_train_lr = accuracy_score(y_pred_train_lr, y_train)\n",
    " \n",
    "accuracy_test_lr = accuracy_score(y_pred_test_lr, y_test)\n",
    " \n",
    "print('Logistic Regression - accuracy_train:', accuracy_train_lr)\n",
    "print('Logistic Regression - accuracy_test:', accuracy_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f76e7",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36273133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, SimpleRNN, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=max_len, padding=\"post\")\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed666004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "#     SimpleRNN(128, return_sequences=True),\n",
    "#     Dropout(0.5),\n",
    "#     SimpleRNN(64, return_sequences=False),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding with slightly larger dimensions for richer word representation\n",
    "    Embedding(input_dim=max_words, output_dim=200, input_length=max_len),\n",
    "    \n",
    "    # First RNN with increased units and recurrent dropout for regularization\n",
    "    SimpleRNN(256, return_sequences=True, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),  # Added before dropout for more stable training\n",
    "    Dropout(0.3),  # Reduced slightly from 0.5 to avoid underfitting\n",
    "    \n",
    "    # Second RNN with additional regularization\n",
    "    SimpleRNN(128, return_sequences=False, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Multiple dense layers with gradually decreasing units for better feature extraction\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get(\"accuracy\") > 92 and logs.get(\"val_accuracy\") > 0.92):\n",
    "            print(\"\\nProses training dihentikan karena Akurasi telah melampaui 92%\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_callback = CustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2325159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m 49/225\u001b[0m \u001b[32mâ”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m27s\u001b[0m 155ms/step - accuracy: 0.3439 - loss: 1.4354"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m early_stopping = EarlyStopping(monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, patience=\u001b[32m3\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m model_checkpoint = ModelCheckpoint(\u001b[33m\"\u001b[39m\u001b[33mbest_model.h5\u001b[39m\u001b[33m\"\u001b[39m, monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m history1 = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_train_pad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m        \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Folder\\Kuliah\\Learn\\Bangkit Mentor\\Machine Learning\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\"best_model_rnn.h5\", monitor=\"val_loss\", save_best_only=True)\n",
    "\n",
    "history1 = model.fit(\n",
    "    x_train_pad,\n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=64, \n",
    "    validation_data=(x_test_pad, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[model_checkpoint, custom_callback]        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68c447",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ad810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_10                â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_19          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_11                â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_20          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_21          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_10                â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)                 â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_19          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_11                â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)                 â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_20          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)                â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_21          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_14 (\u001b[38;5;33mDense\u001b[0m)                â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lstm = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(\n",
    "        input_dim=max_words, \n",
    "        output_dim=embedding_dim, \n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    ),\n",
    "    \n",
    "    # First Bidirectional LSTM layer\n",
    "    Bidirectional(\n",
    "        LSTM(\n",
    "            128, \n",
    "            return_sequences=True, \n",
    "            dropout=0.2,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "        )\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Second Bidirectional LSTM layer\n",
    "    Bidirectional(\n",
    "        LSTM(\n",
    "            64, \n",
    "            return_sequences=False, \n",
    "            dropout=0.2\n",
    "        )\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Fully connected dense layer\n",
    "    Dense(\n",
    "        64, \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient clipping helps with exploding gradients\n",
    "model_lstm.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e57fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a5a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.9689 - loss: 0.1467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 263ms/step - accuracy: 0.9689 - loss: 0.1468 - val_accuracy: 0.8747 - val_loss: 0.5055\n",
      "Epoch 2/10\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.9459 - loss: 0.2137"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\"best_model_lstm.h5\", monitor=\"val_loss\", save_best_only=True)\n",
    "\n",
    "history2 = model_lstm.fit(\n",
    "    x_train_pad,\n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    validation_data=(x_test_pad, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[model_checkpoint, custom_callback],\n",
    "    class_weight=class_weights    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5110011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.8642 - loss: 0.5713\n",
      "Train Accuracy: 0.9804\n",
      "Test Accuracy: 0.8669\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_lstm.evaluate(x_test_pad, y_test)\n",
    "train_acc = history2.history[\"accuracy\"][-1]\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623db53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
